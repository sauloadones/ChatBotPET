{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "99173d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a41195",
   "metadata": {},
   "source": [
    "Leitura do CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8ddae875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dados/dados_transformados.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1a4ca04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Texto</th>\n",
       "      <th>Origem</th>\n",
       "      <th>url</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>Editora</th>\n",
       "      <th>Site Editora</th>\n",
       "      <th>Editora_cod</th>\n",
       "      <th>dominio</th>\n",
       "      <th>dominio_cod</th>\n",
       "      <th>Site_Editora_cod</th>\n",
       "      <th>origem_simplificada</th>\n",
       "      <th>origem_cod</th>\n",
       "      <th>conteudo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vídeo com vaias a Nando Reis teve áudio manipu...</td>\n",
       "      <td>“Nando Reis se F… Se F…. no Rock in Rio”</td>\n",
       "      <td>pessoa</td>\n",
       "      <td>https://lupa.uol.com.br/jornalismo/2022/09/09/...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lupa - UOL</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>5</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Pessoa</td>\n",
       "      <td>8</td>\n",
       "      <td>Vídeo com vaias a Nando Reis teve áudio manipu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Documentos confirmam uso de dinheiro vivo na c...</td>\n",
       "      <td>“Não foi dinheiro vivo. Moeda corrente, quer d...</td>\n",
       "      <td>Cristina Graeml</td>\n",
       "      <td>https://lupa.uol.com.br/jornalismo/2022/09/09/...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lupa - UOL</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>5</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Outros</td>\n",
       "      <td>7</td>\n",
       "      <td>Documentos confirmam uso de dinheiro vivo na c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G1 não publicou notícia de que rainha Elizabet...</td>\n",
       "      <td>“Em seus últimos momentos de vida, rainha Eliz...</td>\n",
       "      <td>Imagem no WhatsApp</td>\n",
       "      <td>https://lupa.uol.com.br/jornalismo/2022/09/08/...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lupa - UOL</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>5</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>WhatsApp</td>\n",
       "      <td>11</td>\n",
       "      <td>G1 não publicou notícia de que rainha Elizabet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TSE não proibiu governo de tomar medidas para ...</td>\n",
       "      <td>“TSE MARCOU PARA PRÓXIMA SEMANA JULGAMENTO OND...</td>\n",
       "      <td>pessoa</td>\n",
       "      <td>https://lupa.uol.com.br/jornalismo/2022/09/08/...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lupa - UOL</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>5</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>Pessoa</td>\n",
       "      <td>8</td>\n",
       "      <td>TSE não proibiu governo de tomar medidas para ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>É falso que órgão de saúde dos EUA passou a re...</td>\n",
       "      <td>De repente, a Ivermectina aparece no site do N...</td>\n",
       "      <td>Post no WhatsApp</td>\n",
       "      <td>https://lupa.uol.com.br/jornalismo/2022/09/08/...</td>\n",
       "      <td>1</td>\n",
       "      <td>Lupa - UOL</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>5</td>\n",
       "      <td>lupa.uol.com.br</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>WhatsApp</td>\n",
       "      <td>11</td>\n",
       "      <td>É falso que órgão de saúde dos EUA passou a re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Titulo  \\\n",
       "0  Vídeo com vaias a Nando Reis teve áudio manipu...   \n",
       "1  Documentos confirmam uso de dinheiro vivo na c...   \n",
       "2  G1 não publicou notícia de que rainha Elizabet...   \n",
       "3  TSE não proibiu governo de tomar medidas para ...   \n",
       "4  É falso que órgão de saúde dos EUA passou a re...   \n",
       "\n",
       "                                               Texto              Origem  \\\n",
       "0           “Nando Reis se F… Se F…. no Rock in Rio”              pessoa   \n",
       "1  “Não foi dinheiro vivo. Moeda corrente, quer d...     Cristina Graeml   \n",
       "2  “Em seus últimos momentos de vida, rainha Eliz...  Imagem no WhatsApp   \n",
       "3  “TSE MARCOU PARA PRÓXIMA SEMANA JULGAMENTO OND...              pessoa   \n",
       "4  De repente, a Ivermectina aparece no site do N...    Post no WhatsApp   \n",
       "\n",
       "                                                 url  Classificacao  \\\n",
       "0  https://lupa.uol.com.br/jornalismo/2022/09/09/...              1   \n",
       "1  https://lupa.uol.com.br/jornalismo/2022/09/09/...              1   \n",
       "2  https://lupa.uol.com.br/jornalismo/2022/09/08/...              1   \n",
       "3  https://lupa.uol.com.br/jornalismo/2022/09/08/...              1   \n",
       "4  https://lupa.uol.com.br/jornalismo/2022/09/08/...              1   \n",
       "\n",
       "      Editora     Site Editora  Editora_cod          dominio  dominio_cod  \\\n",
       "0  Lupa - UOL  lupa.uol.com.br            5  lupa.uol.com.br            2   \n",
       "1  Lupa - UOL  lupa.uol.com.br            5  lupa.uol.com.br            2   \n",
       "2  Lupa - UOL  lupa.uol.com.br            5  lupa.uol.com.br            2   \n",
       "3  Lupa - UOL  lupa.uol.com.br            5  lupa.uol.com.br            2   \n",
       "4  Lupa - UOL  lupa.uol.com.br            5  lupa.uol.com.br            2   \n",
       "\n",
       "   Site_Editora_cod origem_simplificada  origem_cod  \\\n",
       "0                 5              Pessoa           8   \n",
       "1                 5              Outros           7   \n",
       "2                 5            WhatsApp          11   \n",
       "3                 5              Pessoa           8   \n",
       "4                 5            WhatsApp          11   \n",
       "\n",
       "                                            conteudo  \n",
       "0  Vídeo com vaias a Nando Reis teve áudio manipu...  \n",
       "1  Documentos confirmam uso de dinheiro vivo na c...  \n",
       "2  G1 não publicou notícia de que rainha Elizabet...  \n",
       "3  TSE não proibiu governo de tomar medidas para ...  \n",
       "4  É falso que órgão de saúde dos EUA passou a re...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe124c7",
   "metadata": {},
   "source": [
    "Aqui estamos usando o TFidVectorizer para transformar textos em numeros, transformando uma coleção de texto em uma matriz indicando a importancia do peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2e149514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Titulo                 object\n",
       "Texto                  object\n",
       "Origem                 object\n",
       "url                    object\n",
       "Classificacao           int64\n",
       "Editora                object\n",
       "Site Editora           object\n",
       "Editora_cod             int64\n",
       "dominio                object\n",
       "dominio_cod             int64\n",
       "Site_Editora_cod        int64\n",
       "origem_simplificada    object\n",
       "origem_cod              int64\n",
       "conteudo               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_texto = vectorizer.fit_transform(df['conteudo'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6db6014",
   "metadata": {},
   "source": [
    "Aqui estamos juntando as tabelas que serão usadas para o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "872eb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_outros = df[['dominio_cod', 'Editora_cod', 'Site_Editora_cod', 'origem_cod']].values\n",
    "\n",
    "X_final = hstack([X_texto, csr_matrix(X_outros)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bfdb0",
   "metadata": {},
   "source": [
    "Aqui definimos o vetor alvo, e tambem aplicamos o metodo do SMOTE para corrigir o desbalanceamento de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8c630425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Classificacao']\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_final, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48f4806",
   "metadata": {},
   "source": [
    "Aqui dividimos os dados balanceados em conjunto de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d4aafe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
    "    X_res, y_res, test_size=0.3, stratify=y_res, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e33b34e",
   "metadata": {},
   "source": [
    "Treinando o Modelo do RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c972a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_modelo = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight='balanced',  # ← MUITO IMPORTANTE!\n",
    "    random_state=42\n",
    ")\n",
    "rf_modelo.fit(X_treino, y_treino)\n",
    "rf_pred = rf_modelo.predict(X_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ae6ec",
   "metadata": {},
   "source": [
    "Treinando Logistica de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "a8f116ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistica Regressão\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90      6144\n",
      "           1       0.96      0.82      0.88      6143\n",
      "\n",
      "    accuracy                           0.89     12287\n",
      "   macro avg       0.90      0.89      0.89     12287\n",
      "weighted avg       0.90      0.89      0.89     12287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_modelo = LogisticRegression(\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    solver='liblinear',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lr_modelo.fit(X_treino, y_treino)\n",
    "lr_pred = lr_modelo.predict(X_teste)\n",
    "\n",
    "print(\"Logistica Regressão\")\n",
    "print(classification_report(y_teste, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ba4f",
   "metadata": {},
   "source": [
    "Treinando o XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744e031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Saulo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:36:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Verdadeiro       0.94      0.90      0.92      6144\n",
      "       Falso       0.91      0.94      0.92      6143\n",
      "\n",
      "    accuracy                           0.92     12287\n",
      "   macro avg       0.92      0.92      0.92     12287\n",
      "weighted avg       0.92      0.92      0.92     12287\n",
      "\n",
      "[[5538  606]\n",
      " [ 343 5800]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_res == 0).sum() / (y_res == 1).sum(),\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_treino, y_treino)\n",
    "xgb_pred = xgb_model.predict(X_teste)\n",
    "print(classification_report(y_teste, xgb_pred, target_names=[\"Verdadeiro\", \"Falso\"]))\n",
    "print(confusion_matrix(y_teste, xgb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Aqui pegamos a media da Logistica de Regressão e RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aab450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia - Random Forest: 0.9469\n",
      "Acurácia - Logistic Regression: 0.8930\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf_acc = accuracy_score(y_teste, rf_pred)\n",
    "print(f\"Acurácia - Random Forest: {rf_acc:.4f}\")\n",
    "\n",
    "lr_acc = accuracy_score(y_teste, lr_pred)\n",
    "print(f\"Acurácia - Logistic Regression: {lr_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef516a4",
   "metadata": {},
   "source": [
    "Realizamos a validação cruzada 30x sobre o conjuto balanceado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dd8352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_res == 0).sum() / (y_res == 1).sum(),  # balanceamento\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_scores = cross_val_score(xgb_model, X_res, y_res, cv=30, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf8767",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_modelo = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)\n",
    "rf_scores = cross_val_score(rf_modelo, X_res, y_res, cv=30, scoring='accuracy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "64e90b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "lr_modelo = LogisticRegression(max_iter=10000, solver='liblinear', random_state=42)\n",
    "lr_scores = cross_val_score(rf_modelo, X_res, y_res, cv=30, scoring='accuracy')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35026add",
   "metadata": {},
   "source": [
    "Aqui separamos a media das acuraciais e pegamos o melhor modelo para ser usado na nossa aplicacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7862201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Média: 0.9113 ± 0.1444\n",
      "Random Forest - Média: 0.9429 ± 0.0731\n",
      "Logistic Regression - Média: 0.9429 ± 0.0731\n"
     ]
    }
   ],
   "source": [
    "melhor_modelo = max(\n",
    "    [('Logistic Regression', lr_modelo, lr_scores.mean()),\n",
    "     ('Random Forest', rf_modelo, rf_scores.mean()),\n",
    "     ('XGBoost', xgb_model, xgb_scores.mean())],\n",
    "    key=lambda x: x[2]\n",
    ")\n",
    "\n",
    "modelo_final = melhor_modelo[1]\n",
    "modelo_final.fit(X_treino, y_treino) \n",
    "y_pred = modelo_final.predict(X_teste)\n",
    "\n",
    "print(f\"XGBoost - Média: {xgb_scores.mean():.4f} ± {xgb_scores.std():.4f}\")\n",
    "print(f\"Random Forest - Média: {rf_scores.mean():.4f} ± {rf_scores.std():.4f}\")\n",
    "print(f\"Logistic Regression - Média: {lr_scores.mean():.4f} ± {lr_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb9f0b",
   "metadata": {},
   "source": [
    "Aqui importamos o modelo fianl e o vectorizer, eu não usei a biblioteca pickle aqui pois estava dando erro de depuração com questões de atualização de biblioteca e outras bibliotecas envolvidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d7b10642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../modelo/vectorizer_tfidf.joblib']"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(modelo_final, '../modelo/modelo_treinado.joblib')\n",
    "joblib.dump(vectorizer, '../modelo/vectorizer_tfidf.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
